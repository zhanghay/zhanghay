<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/608189 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <meta name="content-class" content="yinxiang.markdown"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 12pt;
    }
  </style>
</head>
<body>
<a name="5468"/>

<div><span><div style="font-size: 14px; margin: 0; padding: 0; width: 100%;"><p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><img src="figures/image-0.png" style="line-height: 160%; margin: 4px 0 10px; box-sizing: border-box; vertical-align: top; max-width: 100%;"></img></p>
<h1 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 41px; border-bottom: 3px double #999; color: #000; margin-top: 14px;">基于 InternLM 和 LangChain 搭建你的知识库</h1>
<div style="line-height: 160%; box-sizing: content-box;"><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">基于 InternLM 和 LangChain 搭建你的知识库
</a><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333; margin-top: 0; margin-bottom: 0;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1 环境配置
</a><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333; margin-top: 0; margin-bottom: 0;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1.1 InternLM 模型部署
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1.2 模型下载
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1.3 LangChain 相关环境配置
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1.4 下载 NLTK 相关资源
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">1.5 下载本项目代码
</a></li></ul></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">2 知识库搭建
</a><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333; margin-top: 0; margin-bottom: 0;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">2.1 数据收集
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">2.2 加载数据
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">2.3 构建向量数据库
</a></li></ul></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">3 InternLM 接入 LangChain
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">4 构建检索问答链
</a><ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333; margin-top: 0; margin-bottom: 0;"><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">4.1 加载向量数据库
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">4.2 实例化自定义 LLM 与 Prompt Template
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">4.3 构建检索问答链
</a></li></ul></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">5 部署 Web Demo
</a></li><li style="line-height: 160%; box-sizing: content-box; position: relative;"><a style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">6 作业
</a></li></ul></li></ul></div><h2 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 34px; border-bottom: 1px solid #dbdbdb; color: #333;">1 环境配置</h2>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">1.1 InternLM 模型部署</h3>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #9b9b9b; line-height: 160%; box-sizing: content-box;">#</span><span style="line-height: 160%; box-sizing: content-box;"> 升级pip</span>
python -m pip install --upgrade pip
pip install modelscope==1.9.5
pip install transformers==4.35.2
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
</code></pre>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">1.2 模型下载</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在本地的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">/root/share/temp/model_repos/internlm-chat-7b</code> 目录下已存储有所需的模型文件参数，可以直接拷贝到个人目录的模型保存地址：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;">mkdir -p /root/data/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b
</code></pre>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">1.3 LangChain 相关环境配置</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在已完成 InternLM 的部署基础上，还需要安装以下依赖包：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;">pip install langchain==0.0.292
pip install gradio==4.4.0
pip install chromadb==0.4.15
pip install sentence-transformers==2.2.2
pip install unstructured==0.10.30
pip install markdown==3.3.7
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">同时，我们需要使用到开源词向量模型 <a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Sentence Transformer</a>:（我们也可以选用别的开源词向量模型来进行 Embedding，目前选用这个模型是相对轻量、支持中文且效果较好的，同学们可以自由尝试别的开源词向量模型）<br/>
首先需要使用 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">huggingface</code> 官方提供的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">huggingface-cli</code> 命令行工具。安装依赖:</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;">pip install -U huggingface_hub
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">然后在和 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">/root/data</code> 目录下新建python文件 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">download_hf.py</code>，填入以下代码：</p>
<ul style="line-height: 160%; box-sizing: content-box; display: block; list-style-type: disc; padding-left: 30px; margin: 6px 0 10px; color: #333;">
<li style="line-height: 160%; box-sizing: content-box; position: relative;">resume-download：断点续下</li>
<li style="line-height: 160%; box-sizing: content-box; position: relative;">local-dir：本地存储路径。（linux环境下需要填写绝对路径）</li>
</ul>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> os
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 下载模型</span>
os.system(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/data/model/sentence-transformer'</span>)
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">但是，使用 huggingface 下载可能速度较慢，我们可以使用 huggingface 镜像下载。与使用hugginge face下载相同，只需要填入镜像地址即可。<br/>
将 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">download_hf.py</code> 中的代码修改为以下代码：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> os
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 设置环境变量</span>
os.environ[<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'HF_ENDPOINT'</span>] = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'https://hf-mirror.com'</span>
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 下载模型</span>
os.system(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/data/model/sentence-transformer'</span>)
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">然后，在 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">/root/data</code> 目录下执行该脚本即可自动开始下载：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;">python download_hf.py
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">更多关于镜像使用可以移步至 <a href="https://hf-mirror.com/" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">HF Mirror</a> 查看。</p>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">1.4 下载 NLTK 相关资源</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">我们在使用开源词向量模型构建开源词向量的时候，需要用到第三方库 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">nltk</code> 的一些资源。正常情况下，其会自动从互联网上下载，但可能由于网络原因会导致下载中断，此处我们可以从国内仓库镜像地址下载相关资源，保存到服务器上。<br/>
我们用以下命令下载 nltk 资源并解压到服务器上：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">cd</span> /root
git <span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">clone</span> https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages
<span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">cd</span> nltk_data
mv packages/*  ./
<span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">cd</span> tokenizers
unzip punkt.zip
<span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">cd</span> ../taggers
unzip averaged_perceptron_tagger.zip
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">之后使用时服务器即会自动使用已有资源，无需再次下载。</p>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">1.5 下载本项目代码</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">我们在仓库中同步提供了所有脚本，可以查看该教程文件的同级目录的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">demo</code> 文件夹。<br/>
建议通过以下目录将仓库 clone 到本地，可以直接在本地运行相关代码：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;">cd /root/data
git clone https://github.com/InternLM/tutorial
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">通过上述命令，可以将本仓库 clone 到本地 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">root/data/tutorial</code> 目录下，在之后的过程中可以对照仓库中的脚本来完成自己的代码，也可以直接使用仓库中的脚本。</p>
<h2 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 34px; border-bottom: 1px solid #dbdbdb; color: #333;">2 知识库搭建</h2>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">2.1 数据收集</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">我们选择由上海人工智能实验室开源的一系列大模型工具开源仓库作为语料库来源，包括：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 进入到数据库盘</span>
<span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">cd</span> /root/data
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># clone 上述开源仓库</span>
git <span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">clone</span> https://gitee.com/open-compass/opencompass.git
git <span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">clone</span> https://gitee.com/InternLM/lmdeploy.git
git <span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">clone</span> https://gitee.com/InternLM/xtuner.git
git <span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">clone</span> https://gitee.com/InternLM/InternLM-XComposer.git
git <span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">clone</span> https://gitee.com/InternLM/lagent.git
git <span style="color: #4ec9b0; line-height: 160%; box-sizing: content-box;">clone</span> https://gitee.com/InternLM/InternLM.git
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">接着，为语料处理方便，我们将选用上述仓库中所有的 markdown、txt 文件作为示例语料库。注意，也可以选用其中的代码文件加入到知识库中，但需要针对代码文件格式进行额外处理（因为代码文件对逻辑联系要求较高，且规范性较强，在分割时最好基于代码模块进行分割再加入向量数据库）。<br/>
我们首先将上述仓库中所有满足条件的文件路径找出来，我们定义一个函数，该函数将递归指定文件夹路径，返回其中所有满足条件（即后缀名为 .md 或者 .txt 的文件）的文件路径：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> os
<span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">get_files</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(dir_path)</span>:</span>
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># args：dir_path，目标文件夹路径</span>
    file_list = []
    <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">for</span> filepath, dirnames, filenames <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">in</span> os.walk(dir_path):
        <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># os.walk 函数将递归遍历指定文件夹</span>
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">for</span> filename <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">in</span> filenames:
            <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 通过后缀名判断文件类型是否满足要求</span>
            <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">if</span> filename.endswith(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;.md&quot;</span>):
                <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 如果满足要求，将其绝对路径加入到结果列表</span>
                file_list.append(os.path.join(filepath, filename))
            <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">elif</span> filename.endswith(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;.txt&quot;</span>):
                file_list.append(os.path.join(filepath, filename))
    <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> file_list
</code></pre>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">2.2 加载数据</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">得到所有目标文件路径之后，我们可以使用 LangChain 提供的 FileLoader 对象来加载目标文件，得到由目标文件解析出的纯文本内容。由于不同类型的文件需要对应不同的 FileLoader，我们判断目标文件类型，并针对性调用对应类型的 FileLoader，同时，调用 FileLoader 对象的 load 方法来得到加载之后的纯文本对象：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> tqdm <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> tqdm
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.document_loaders <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> UnstructuredFileLoader
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.document_loaders <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> UnstructuredMarkdownLoader
<span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">get_text</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(dir_path)</span>:</span>
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># args：dir_path，目标文件夹路径</span>
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 首先调用上文定义的函数得到目标文件路径列表</span>
    file_lst = get_files(dir_path)
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># docs 存放加载之后的纯文本对象</span>
    docs = []
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 遍历所有目标文件</span>
    <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">for</span> one_file <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">in</span> tqdm(file_lst):
        file_type = one_file.split(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'.'</span>)[<span style="color: #b8d7a3; line-height: 160%; box-sizing: content-box;">-1</span>]
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">if</span> file_type == <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'md'</span>:
            loader = UnstructuredMarkdownLoader(one_file)
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">elif</span> file_type == <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'txt'</span>:
            loader = UnstructuredFileLoader(one_file)
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">else</span>:
            <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 如果是不符合条件的文件，直接跳过</span>
            <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">continue</span>
        docs.extend(loader.load())
    <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> docs
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">使用上文函数，我们得到的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">docs</code> 为一个纯文本对象对应的列表。</p>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">2.3 构建向量数据库</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">得到该列表之后，我们就可以将它引入到 LangChain 框架中构建向量数据库。由纯文本对象构建向量数据库，我们需要先对文本进行分块，接着对文本块进行向量化。<br/>
LangChain 提供了多种文本分块工具，此处我们使用字符串递归分割器，并选择分块大小为 500，块重叠长度为 150</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.text_splitter <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=<span style="color: #b8d7a3; line-height: 160%; box-sizing: content-box;">500</span>, chunk_overlap=<span style="color: #b8d7a3; line-height: 160%; box-sizing: content-box;">150</span>)
split_docs = text_splitter.split_documents(docs)
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">接着我们选用开源词向量模型 <a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">Sentence Transformer</a> 来进行文本向量化。LangChain 提供了直接引入 HuggingFace 开源社区中的模型进行向量化的接口：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.embeddings.huggingface <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name=<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;/root/data/model/sentence-transformer&quot;</span>)
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">同时，考虑到 Chroma 是目前最常用的入门数据库，我们选择 Chroma 作为向量数据库，基于上文分块后的文档以及加载的开源向量化模型，将语料加载到指定路径下的向量数据库：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.vectorstores <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> Chroma
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 定义持久化路径</span>
persist_directory = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'data_base/vector_db/chroma'</span>
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 加载数据库</span>
vectordb = Chroma.from_documents(
    documents=split_docs,
    embedding=embeddings,
    persist_directory=persist_directory  <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 允许我们将persist_directory目录保存到磁盘上</span>
)
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 将加载的向量数据库持久化到磁盘上</span>
vectordb.persist()
</code></pre>
<h2 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 34px; border-bottom: 1px solid #dbdbdb; color: #333;">3 InternLM 接入 LangChain</h2>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">为便捷构建 LLM 应用，我们需要基于本地部署的 InternLM，继承 LangChain 的 LLM 类自定义一个 InternLM LLM 子类，从而实现将 InternLM 接入到 LangChain 框架中。完成 LangChain 的自定义 LLM 子类之后，可以以完全一致的方式调用 LangChain 的接口，而无需考虑底层模型调用的不一致。<br/>
基于本地部署的 InternLM 自定义 LLM 类并不复杂，我们只需从 LangChain.llms.base.LLM 类继承一个子类，并重写构造函数与 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">_call</code> 函数即可：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.llms.base <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> LLM
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> typing <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> Any, List, Optional
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.callbacks.manager <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> CallbackManagerForLLMRun
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> transformers <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> AutoTokenizer, AutoModelForCausalLM
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> torch
<span style="color: #b8d7a3; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">class</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">InternLM_LLM</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(LLM)</span>:</span>
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 基于本地 InternLM 自定义 LLM 类</span>
    tokenizer : AutoTokenizer = <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">None</span>
    model: AutoModelForCausalLM = <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">None</span>
    <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">__init__</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(self, model_path :str)</span>:</span>
        <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># model_path: InternLM 模型路径</span>
        <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 从本地初始化模型</span>
        super().__init__()
        print(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;正在从本地加载模型...&quot;</span>)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">True</span>)
        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">True</span>).to(torch.bfloat16).cuda()
        self.model = self.model.eval()
        print(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;完成本地模型的加载&quot;</span>)
    <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">_call</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(self, prompt : str, stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any)</span>:</span>
        <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 重写调用函数</span>
        system_prompt = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).
        - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.
        - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.
        &quot;&quot;&quot;</span>
       
        messages = [(system_prompt, <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">''</span>)]
        response, history = self.model.chat(self.tokenizer, prompt , history=messages)
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> response
       
    @property
    <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">_llm_type</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(self)</span> -&gt; str:</span>
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;InternLM&quot;</span>
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在上述类定义中，我们分别重写了构造函数和 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">_call</code> 函数：对于构造函数，我们在对象实例化的一开始加载本地部署的 InternLM 模型，从而避免每一次调用都需要重新加载模型带来的时间过长；<code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">_call</code> 函数是 LLM 类的核心函数，LangChain 会调用该函数来调用 LLM，在该函数中，我们调用已实例化模型的 chat 方法，从而实现对模型的调用并返回调用结果。<br/>
在整体项目中，我们将上述代码封装为 LLM.py，后续将直接从该文件中引入自定义的 LLM 类。</p>
<h2 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 34px; border-bottom: 1px solid #dbdbdb; color: #333;">4 构建检索问答链</h2>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">调用一个 LangChain 提供的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">RetrievalQA</code> 对象，通过初始化时填入已构建的数据库和自定义 LLM 作为参数，来简便地完成检索增强问答的全流程，LangChain 会自动完成基于用户提问进行检索、获取相关文档、拼接为合适的 Prompt 并交给 LLM 问答的全部流程。</p>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">4.1 加载向量数据库</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">首先我们需要将上文构建的向量数据库导入进来，我们可以直接通过 Chroma 以及上文定义的词向量模型来加载已构建的数据库：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.vectorstores <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> Chroma
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.embeddings.huggingface <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> HuggingFaceEmbeddings
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> os
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 定义 Embeddings</span>
embeddings = HuggingFaceEmbeddings(model_name=<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;/root/data/model/sentence-transformer&quot;</span>)
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 向量数据库持久化路径</span>
persist_directory = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'data_base/vector_db/chroma'</span>
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 加载数据库</span>
vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embeddings
)
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">上述代码得到的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">vectordb</code> 对象即为我们已构建的向量数据库对象，该对象可以针对用户的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">query</code> 进行语义向量检索，得到与用户提问相关的知识片段。</p>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">4.2 实例化自定义 LLM 与 Prompt Template</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">接着，我们实例化一个基于 InternLM 自定义的 LLM 对象：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> LLM <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> InternLM_LLM
llm = InternLM_LLM(model_path = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)
llm.predict(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;你是谁&quot;</span>)
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">构建检索问答链，还需要构建一个 Prompt Template，该 Template 其实基于一个带变量的字符串，在检索之后，LangChain 会将检索到的相关文档片段填入到 Template 的变量中，从而实现带知识的 Prompt 构建。我们可以基于 LangChain 的 Template 基类来实例化这样一个 Template 对象：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.prompts <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> PromptTemplate
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 我们所构造的 Prompt 模板</span>
template = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;&quot;&quot;使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。
问题: {question}
可参考的上下文：
···
{context}
···
如果给定的上下文无法让你做出回答，请回答你不知道。
有用的回答:&quot;&quot;&quot;</span>
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充</span>
QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;context&quot;</span>,<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;question&quot;</span>],template=template)
</code></pre>
<h3 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 27px; color: #333;">4.3 构建检索问答链</h3>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">最后，可以调用 LangChain 提供的检索问答链构造函数，基于我们的自定义 LLM、Prompt Template 和向量知识库来构建一个基于 InternLM 的检索问答链：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.chains <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> RetrievalQA
qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">True</span>,chain_type_kwargs={<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT})
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">得到的 <code style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; color: #c1788b; padding: 4px 4px 2px 0; letter-spacing: -.3px;">qa_chain</code> 对象即可以实现我们的核心功能，即基于 InternLM 模型的专业知识库助手。我们可以对比该检索问答链和纯 LLM 的问答效果：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 检索问答链回答效果</span>
question = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;什么是InternLM&quot;</span>
result = qa_chain({<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;query&quot;</span>: question})
print(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;检索问答链回答 question 的结果：&quot;</span>)
print(result[<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;result&quot;</span>])
<span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 仅 LLM 回答效果</span>
result_2 = llm(question)
print(<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;大模型回答 question 的结果：&quot;</span>)
print(result_2)
</code></pre>
<h2 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 34px; border-bottom: 1px solid #dbdbdb; color: #333;">5 部署 Web Demo</h2>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在完成上述核心功能后，我们可以基于 Gradio 框架将其部署到 Web 网页，从而搭建一个小型 Demo，便于测试与使用。<br/>
我们首先将上文的代码内容封装为一个返回构建的检索问答链对象的函数，并在启动 Gradio 的第一时间调用该函数得到检索问答链对象，后续直接使用该对象进行问答对话，从而避免重复加载模型：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.vectorstores <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> Chroma
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.embeddings.huggingface <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> HuggingFaceEmbeddings
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> os
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> LLM <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> InternLM_LLM
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.prompts <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> PromptTemplate
<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">from</span> langchain.chains <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">import</span> RetrievalQA
<span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">load_chain</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">()</span>:</span>
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 加载问答链</span>
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 定义 Embeddings</span>
    embeddings = HuggingFaceEmbeddings(model_name=<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;/root/data/model/sentence-transformer&quot;</span>)
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 向量数据库持久化路径</span>
    persist_directory = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">'data_base/vector_db/chroma'</span>
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 加载数据库</span>
    vectordb = Chroma(
        persist_directory=persist_directory,  <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 允许我们将persist_directory目录保存到磁盘上</span>
        embedding_function=embeddings
    )
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 加载自定义 LLM</span>
    llm = InternLM_LLM(model_path = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 定义一个 Prompt Template</span>
    template = <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;&quot;&quot;使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答
    案。尽量使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。
    {context}
    问题: {question}
    有用的回答:&quot;&quot;&quot;</span>
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;context&quot;</span>,<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;question&quot;</span>],template=template)
    <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 运行 chain</span>
    qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=<span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">True</span>,chain_type_kwargs={<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT})
   
    <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> qa_chain
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">接着我们定义一个类，该类负责加载并存储检索问答链，并响应 Web 界面里调用检索问答链进行回答的动作：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;"><span style="color: #b8d7a3; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">class</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">Model_center</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">()</span>:</span>
    <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;&quot;&quot;
    存储检索问答链的对象
    &quot;&quot;&quot;</span>
    <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">__init__</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(self)</span>:</span>
        <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 构造函数，加载检索问答链</span>
        self.chain = load_chain()
    <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;"><span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">def</span> <span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">qa_chain_self_answer</span><span style="color: #dcdcdc; line-height: 160%; box-sizing: content-box;">(self, question: str, chat_history: list = [])</span>:</span>
        <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;&quot;&quot;
        调用问答链进行回答
        &quot;&quot;&quot;</span>
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">if</span> question == <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">None</span> <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">or</span> len(question) &lt; <span style="color: #b8d7a3; line-height: 160%; box-sizing: content-box;">1</span>:
            <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;&quot;</span>, chat_history
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">try</span>:
            chat_history.append(
                (question, self.chain({<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;query&quot;</span>: question})[<span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;result&quot;</span>]))
            <span style="color: #57a64a; font-style: italic; line-height: 160%; box-sizing: content-box;"># 将问答结果直接附加到问答历史中，Gradio 会将其展示出来</span>
            <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> <span style="color: #d69d85; line-height: 160%; box-sizing: content-box;">&quot;&quot;</span>, chat_history
        <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">except</span> Exception <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">as</span> e:
            <span style="color: #569cd6; line-height: 160%; box-sizing: content-box;">return</span> e, chat_history
</code></pre>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">然后我们只需按照 Gradio 的框架使用方法，实例化一个 Web 界面并将点击动作绑定到上述类的回答方法即可：<br/>
通过将上述代码封装为 run_gradio.py 脚本，直接通过 python 命令运行，即可在本地启动知识库助手的 Web Demo，默认会在 7860 端口运行，接下来将服务器端口映射到本地端口即可访问:</p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">在本地终端输入以下指令.7860是在服务器中打开的端口，而33090是根据开发机的端口进行更改。如下图所示：</p>
<pre style="line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; margin: 2px 0 8px; background-color: #f5f7f8;"><code style="display: block; overflow-x: auto; background: #1e1e1e; line-height: 160%; box-sizing: content-box; border: 0; border-radius: 0; letter-spacing: -.3px; padding: 18px; color: #f4f4f4; white-space: pre-wrap;">ssh -CNg -L 7860:127.0.0.1:7860 root@ssh.intern-ai.org.cn -p 33090
</code></pre>
<h2 style="line-height: 160%; box-sizing: content-box; font-weight: 700; font-size: 34px; border-bottom: 1px solid #dbdbdb; color: #333;">6 作业</h2>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">提交方式：在各个班级对应的 GitHub Discussion 帖子中进行提交。<br/>
<strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">基础作业</strong>：<br/>
复现课程知识库助手搭建过程<br/>
1、构建向量数据库<br/>
<img src="【3浦语实战】LangChain_files/Image.png" type="image/png" data-filename="Image.png"/><br/>
2、qa链<br/>
<img src="【3浦语实战】LangChain_files/Image [1].png" type="image/png" data-filename="Image.png"/><br/>
3、回答<br/>
<img src="【3浦语实战】LangChain_files/Image [2].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;">4、web<br/>
<img src="【3浦语实战】LangChain_files/Image [3].png" type="image/png" data-filename="Image.png"/></p>
<p style="line-height: 160%; box-sizing: content-box; margin: 10px 0; color: #333;"><strong style="line-height: 160%; box-sizing: content-box; font-weight: 700;">进阶作业</strong>：<br/>
选择一个垂直领域，收集该领域的专业资料构建专业知识库，并搭建专业问答助手，并在 <a href="https://openxlab.org.cn/apps" style="line-height: 160%; box-sizing: content-box; text-decoration: underline; color: #5286bc;">OpenXLab</a> 上成功部署（截图，并提供应用地址）</p>
</div><center style="display:none !important;visibility:collapse !important;height:0 !important;white-space:nowrap;width:100%;overflow:hidden">%5BTOC%5D%0A%23%23%201%20%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%0A%23%23%23%201.1%20InternLM%20%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%0A%0A%60%60%60shell%0A%23%20%E5%8D%87%E7%BA%A7pip%0Apython%20-m%20pip%20install%20--upgrade%20pip%0Apip%20install%20modelscope%3D%3D1.9.5%0Apip%20install%20transformers%3D%3D4.35.2%0Apip%20install%20streamlit%3D%3D1.24.0%0Apip%20install%20sentencepiece%3D%3D0.1.99%0Apip%20install%20accelerate%3D%3D0.24.1%0A%60%60%60%0A%23%23%23%201.2%20%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD%0A%E5%9C%A8%E6%9C%AC%E5%9C%B0%E7%9A%84%20%60%2Froot%2Fshare%2Ftemp%2Fmodel_repos%2Finternlm-chat-7b%60%20%E7%9B%AE%E5%BD%95%E4%B8%8B%E5%B7%B2%E5%AD%98%E5%82%A8%E6%9C%89%E6%89%80%E9%9C%80%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E5%8F%82%E6%95%B0%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E6%8B%B7%E8%B4%9D%E5%88%B0%E4%B8%AA%E4%BA%BA%E7%9B%AE%E5%BD%95%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%9C%B0%E5%9D%80%EF%BC%9A%0A%60%60%60bash%0Amkdir%20-p%20%2Froot%2Fdata%2Fmodel%2FShanghai_AI_Laboratory%0Acp%20-r%20%2Froot%2Fshare%2Ftemp%2Fmodel_repos%2Finternlm-chat-7b%20%2Froot%2Fdata%2Fmodel%2FShanghai_AI_Laboratory%2Finternlm-chat-7b%0A%60%60%60%0A%0A%23%23%23%201.3%20LangChain%20%E7%9B%B8%E5%85%B3%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%0A%E5%9C%A8%E5%B7%B2%E5%AE%8C%E6%88%90%20InternLM%20%E7%9A%84%E9%83%A8%E7%BD%B2%E5%9F%BA%E7%A1%80%E4%B8%8A%EF%BC%8C%E8%BF%98%E9%9C%80%E8%A6%81%E5%AE%89%E8%A3%85%E4%BB%A5%E4%B8%8B%E4%BE%9D%E8%B5%96%E5%8C%85%EF%BC%9A%0A%60%60%60bash%0Apip%20install%20langchain%3D%3D0.0.292%0Apip%20install%20gradio%3D%3D4.4.0%0Apip%20install%20chromadb%3D%3D0.4.15%0Apip%20install%20sentence-transformers%3D%3D2.2.2%0Apip%20install%20unstructured%3D%3D0.10.30%0Apip%20install%20markdown%3D%3D3.3.7%0A%60%60%60%0A%E5%90%8C%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E4%BD%BF%E7%94%A8%E5%88%B0%E5%BC%80%E6%BA%90%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%20%5BSentence%20Transformer%5D(https%3A%2F%2Fhuggingface.co%2Fsentence-transformers%2Fparaphrase-multilingual-MiniLM-L12-v2)%3A%EF%BC%88%E6%88%91%E4%BB%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E9%80%89%E7%94%A8%E5%88%AB%E7%9A%84%E5%BC%80%E6%BA%90%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E6%9D%A5%E8%BF%9B%E8%A1%8C%20Embedding%EF%BC%8C%E7%9B%AE%E5%89%8D%E9%80%89%E7%94%A8%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%98%AF%E7%9B%B8%E5%AF%B9%E8%BD%BB%E9%87%8F%E3%80%81%E6%94%AF%E6%8C%81%E4%B8%AD%E6%96%87%E4%B8%94%E6%95%88%E6%9E%9C%E8%BE%83%E5%A5%BD%E7%9A%84%EF%BC%8C%E5%90%8C%E5%AD%A6%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%87%AA%E7%94%B1%E5%B0%9D%E8%AF%95%E5%88%AB%E7%9A%84%E5%BC%80%E6%BA%90%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%EF%BC%89%0A%E9%A6%96%E5%85%88%E9%9C%80%E8%A6%81%E4%BD%BF%E7%94%A8%20%60huggingface%60%20%E5%AE%98%E6%96%B9%E6%8F%90%E4%BE%9B%E7%9A%84%20%60huggingface-cli%60%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7%E3%80%82%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%3A%0A%60%60%60shell%0Apip%20install%20-U%20huggingface_hub%0A%60%60%60%0A%E7%84%B6%E5%90%8E%E5%9C%A8%E5%92%8C%20%60%2Froot%2Fdata%60%20%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%B0%E5%BB%BApython%E6%96%87%E4%BB%B6%20%60download_hf.py%60%EF%BC%8C%E5%A1%AB%E5%85%A5%E4%BB%A5%E4%B8%8B%E4%BB%A3%E7%A0%81%EF%BC%9A%0A-%20resume-download%EF%BC%9A%E6%96%AD%E7%82%B9%E7%BB%AD%E4%B8%8B%0A-%20local-dir%EF%BC%9A%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8%E8%B7%AF%E5%BE%84%E3%80%82%EF%BC%88linux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%9C%80%E8%A6%81%E5%A1%AB%E5%86%99%E7%BB%9D%E5%AF%B9%E8%B7%AF%E5%BE%84%EF%BC%89%0A%60%60%60python%0Aimport%20os%0A%23%20%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%0Aos.system('huggingface-cli%20download%20--resume-download%20sentence-transformers%2Fparaphrase-multilingual-MiniLM-L12-v2%20--local-dir%20%2Froot%2Fdata%2Fmodel%2Fsentence-transformer')%0A%60%60%60%0A%E4%BD%86%E6%98%AF%EF%BC%8C%E4%BD%BF%E7%94%A8%20huggingface%20%E4%B8%8B%E8%BD%BD%E5%8F%AF%E8%83%BD%E9%80%9F%E5%BA%A6%E8%BE%83%E6%85%A2%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%20huggingface%20%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD%E3%80%82%E4%B8%8E%E4%BD%BF%E7%94%A8hugginge%20face%E4%B8%8B%E8%BD%BD%E7%9B%B8%E5%90%8C%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81%E5%A1%AB%E5%85%A5%E9%95%9C%E5%83%8F%E5%9C%B0%E5%9D%80%E5%8D%B3%E5%8F%AF%E3%80%82%0A%E5%B0%86%20%60download_hf.py%60%20%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E4%BF%AE%E6%94%B9%E4%B8%BA%E4%BB%A5%E4%B8%8B%E4%BB%A3%E7%A0%81%EF%BC%9A%0A%60%60%60python%0Aimport%20os%0A%23%20%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%0Aos.environ%5B'HF_ENDPOINT'%5D%20%3D%20'https%3A%2F%2Fhf-mirror.com'%0A%23%20%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%0Aos.system('huggingface-cli%20download%20--resume-download%20sentence-transformers%2Fparaphrase-multilingual-MiniLM-L12-v2%20--local-dir%20%2Froot%2Fdata%2Fmodel%2Fsentence-transformer')%0A%60%60%60%0A%E7%84%B6%E5%90%8E%EF%BC%8C%E5%9C%A8%20%60%2Froot%2Fdata%60%20%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%89%A7%E8%A1%8C%E8%AF%A5%E8%84%9A%E6%9C%AC%E5%8D%B3%E5%8F%AF%E8%87%AA%E5%8A%A8%E5%BC%80%E5%A7%8B%E4%B8%8B%E8%BD%BD%EF%BC%9A%0A%60%60%60bash%0Apython%20download_hf.py%0A%60%60%60%0A%E6%9B%B4%E5%A4%9A%E5%85%B3%E4%BA%8E%E9%95%9C%E5%83%8F%E4%BD%BF%E7%94%A8%E5%8F%AF%E4%BB%A5%E7%A7%BB%E6%AD%A5%E8%87%B3%20%5BHF%20Mirror%5D(https%3A%2F%2Fhf-mirror.com%2F)%20%E6%9F%A5%E7%9C%8B%E3%80%82%0A%23%23%23%201.4%20%E4%B8%8B%E8%BD%BD%20NLTK%20%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%0A%E6%88%91%E4%BB%AC%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%BC%80%E6%BA%90%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E5%BC%80%E6%BA%90%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E9%9C%80%E8%A6%81%E7%94%A8%E5%88%B0%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%20%60nltk%60%20%E7%9A%84%E4%B8%80%E4%BA%9B%E8%B5%84%E6%BA%90%E3%80%82%E6%AD%A3%E5%B8%B8%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%85%B6%E4%BC%9A%E8%87%AA%E5%8A%A8%E4%BB%8E%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%8A%E4%B8%8B%E8%BD%BD%EF%BC%8C%E4%BD%86%E5%8F%AF%E8%83%BD%E7%94%B1%E4%BA%8E%E7%BD%91%E7%BB%9C%E5%8E%9F%E5%9B%A0%E4%BC%9A%E5%AF%BC%E8%87%B4%E4%B8%8B%E8%BD%BD%E4%B8%AD%E6%96%AD%EF%BC%8C%E6%AD%A4%E5%A4%84%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BB%8E%E5%9B%BD%E5%86%85%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F%E5%9C%B0%E5%9D%80%E4%B8%8B%E8%BD%BD%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%EF%BC%8C%E4%BF%9D%E5%AD%98%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E3%80%82%0A%E6%88%91%E4%BB%AC%E7%94%A8%E4%BB%A5%E4%B8%8B%E5%91%BD%E4%BB%A4%E4%B8%8B%E8%BD%BD%20nltk%20%E8%B5%84%E6%BA%90%E5%B9%B6%E8%A7%A3%E5%8E%8B%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%EF%BC%9A%0A%60%60%60bash%0Acd%20%2Froot%0Agit%20clone%20https%3A%2F%2Fgitee.com%2Fyzy0612%2Fnltk_data.git%20%C2%A0--branch%20gh-pages%0Acd%20nltk_data%0Amv%20packages%2F*%20%C2%A0.%2F%0Acd%20tokenizers%0Aunzip%20punkt.zip%0Acd%20..%2Ftaggers%0Aunzip%20averaged_perceptron_tagger.zip%0A%60%60%60%0A%E4%B9%8B%E5%90%8E%E4%BD%BF%E7%94%A8%E6%97%B6%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8D%B3%E4%BC%9A%E8%87%AA%E5%8A%A8%E4%BD%BF%E7%94%A8%E5%B7%B2%E6%9C%89%E8%B5%84%E6%BA%90%EF%BC%8C%E6%97%A0%E9%9C%80%E5%86%8D%E6%AC%A1%E4%B8%8B%E8%BD%BD%E3%80%82%0A%23%23%23%201.5%20%E4%B8%8B%E8%BD%BD%E6%9C%AC%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%0A%E6%88%91%E4%BB%AC%E5%9C%A8%E4%BB%93%E5%BA%93%E4%B8%AD%E5%90%8C%E6%AD%A5%E6%8F%90%E4%BE%9B%E4%BA%86%E6%89%80%E6%9C%89%E8%84%9A%E6%9C%AC%EF%BC%8C%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E8%AF%A5%E6%95%99%E7%A8%8B%E6%96%87%E4%BB%B6%E7%9A%84%E5%90%8C%E7%BA%A7%E7%9B%AE%E5%BD%95%E7%9A%84%20%60demo%60%20%E6%96%87%E4%BB%B6%E5%A4%B9%E3%80%82%0A%E5%BB%BA%E8%AE%AE%E9%80%9A%E8%BF%87%E4%BB%A5%E4%B8%8B%E7%9B%AE%E5%BD%95%E5%B0%86%E4%BB%93%E5%BA%93%20clone%20%E5%88%B0%E6%9C%AC%E5%9C%B0%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%9C%A8%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%EF%BC%9A%0A%60%60%60bahs%0Acd%20%2Froot%2Fdata%0Agit%20clone%20https%3A%2F%2Fgithub.com%2FInternLM%2Ftutorial%0A%60%60%60%0A%E9%80%9A%E8%BF%87%E4%B8%8A%E8%BF%B0%E5%91%BD%E4%BB%A4%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%B0%86%E6%9C%AC%E4%BB%93%E5%BA%93%20clone%20%E5%88%B0%E6%9C%AC%E5%9C%B0%20%60root%2Fdata%2Ftutorial%60%20%E7%9B%AE%E5%BD%95%E4%B8%8B%EF%BC%8C%E5%9C%A8%E4%B9%8B%E5%90%8E%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%8F%AF%E4%BB%A5%E5%AF%B9%E7%85%A7%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E8%84%9A%E6%9C%AC%E6%9D%A5%E5%AE%8C%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E8%84%9A%E6%9C%AC%E3%80%82%0A%23%23%202%20%E7%9F%A5%E8%AF%86%E5%BA%93%E6%90%AD%E5%BB%BA%0A%23%23%23%202.1%20%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%0A%E6%88%91%E4%BB%AC%E9%80%89%E6%8B%A9%E7%94%B1%E4%B8%8A%E6%B5%B7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%BC%80%E6%BA%90%E7%9A%84%E4%B8%80%E7%B3%BB%E5%88%97%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B7%A5%E5%85%B7%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%E4%BD%9C%E4%B8%BA%E8%AF%AD%E6%96%99%E5%BA%93%E6%9D%A5%E6%BA%90%EF%BC%8C%E5%8C%85%E6%8B%AC%EF%BC%9A%0A%0A%60%60%60bash%0A%23%20%E8%BF%9B%E5%85%A5%E5%88%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%98%0Acd%20%2Froot%2Fdata%0A%23%20clone%20%E4%B8%8A%E8%BF%B0%E5%BC%80%E6%BA%90%E4%BB%93%E5%BA%93%0Agit%20clone%20https%3A%2F%2Fgitee.com%2Fopen-compass%2Fopencompass.git%0Agit%20clone%20https%3A%2F%2Fgitee.com%2FInternLM%2Flmdeploy.git%0Agit%20clone%20https%3A%2F%2Fgitee.com%2FInternLM%2Fxtuner.git%0Agit%20clone%20https%3A%2F%2Fgitee.com%2FInternLM%2FInternLM-XComposer.git%0Agit%20clone%20https%3A%2F%2Fgitee.com%2FInternLM%2Flagent.git%0Agit%20clone%20https%3A%2F%2Fgitee.com%2FInternLM%2FInternLM.git%0A%60%60%60%0A%E6%8E%A5%E7%9D%80%EF%BC%8C%E4%B8%BA%E8%AF%AD%E6%96%99%E5%A4%84%E7%90%86%E6%96%B9%E4%BE%BF%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%86%E9%80%89%E7%94%A8%E4%B8%8A%E8%BF%B0%E4%BB%93%E5%BA%93%E4%B8%AD%E6%89%80%E6%9C%89%E7%9A%84%20markdown%E3%80%81txt%20%E6%96%87%E4%BB%B6%E4%BD%9C%E4%B8%BA%E7%A4%BA%E4%BE%8B%E8%AF%AD%E6%96%99%E5%BA%93%E3%80%82%E6%B3%A8%E6%84%8F%EF%BC%8C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E9%80%89%E7%94%A8%E5%85%B6%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E6%96%87%E4%BB%B6%E5%8A%A0%E5%85%A5%E5%88%B0%E7%9F%A5%E8%AF%86%E5%BA%93%E4%B8%AD%EF%BC%8C%E4%BD%86%E9%9C%80%E8%A6%81%E9%92%88%E5%AF%B9%E4%BB%A3%E7%A0%81%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E8%BF%9B%E8%A1%8C%E9%A2%9D%E5%A4%96%E5%A4%84%E7%90%86%EF%BC%88%E5%9B%A0%E4%B8%BA%E4%BB%A3%E7%A0%81%E6%96%87%E4%BB%B6%E5%AF%B9%E9%80%BB%E8%BE%91%E8%81%94%E7%B3%BB%E8%A6%81%E6%B1%82%E8%BE%83%E9%AB%98%EF%BC%8C%E4%B8%94%E8%A7%84%E8%8C%83%E6%80%A7%E8%BE%83%E5%BC%BA%EF%BC%8C%E5%9C%A8%E5%88%86%E5%89%B2%E6%97%B6%E6%9C%80%E5%A5%BD%E5%9F%BA%E4%BA%8E%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E5%88%86%E5%89%B2%E5%86%8D%E5%8A%A0%E5%85%A5%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%89%E3%80%82%0A%E6%88%91%E4%BB%AC%E9%A6%96%E5%85%88%E5%B0%86%E4%B8%8A%E8%BF%B0%E4%BB%93%E5%BA%93%E4%B8%AD%E6%89%80%E6%9C%89%E6%BB%A1%E8%B6%B3%E6%9D%A1%E4%BB%B6%E7%9A%84%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E6%89%BE%E5%87%BA%E6%9D%A5%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%EF%BC%8C%E8%AF%A5%E5%87%BD%E6%95%B0%E5%B0%86%E9%80%92%E5%BD%92%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E5%A4%B9%E8%B7%AF%E5%BE%84%EF%BC%8C%E8%BF%94%E5%9B%9E%E5%85%B6%E4%B8%AD%E6%89%80%E6%9C%89%E6%BB%A1%E8%B6%B3%E6%9D%A1%E4%BB%B6%EF%BC%88%E5%8D%B3%E5%90%8E%E7%BC%80%E5%90%8D%E4%B8%BA%20.md%20%E6%88%96%E8%80%85%20.txt%20%E7%9A%84%E6%96%87%E4%BB%B6%EF%BC%89%E7%9A%84%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%EF%BC%9A%0A%60%60%60python%0Aimport%20os%0Adef%20get_files(dir_path)%3A%0A%C2%A0%20%C2%A0%20%23%20args%EF%BC%9Adir_path%EF%BC%8C%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%B7%AF%E5%BE%84%0A%C2%A0%20%C2%A0%20file_list%20%3D%20%5B%5D%0A%C2%A0%20%C2%A0%20for%20filepath%2C%20dirnames%2C%20filenames%20in%20os.walk(dir_path)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20os.walk%20%E5%87%BD%E6%95%B0%E5%B0%86%E9%80%92%E5%BD%92%E9%81%8D%E5%8E%86%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E5%A4%B9%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20for%20filename%20in%20filenames%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20%E9%80%9A%E8%BF%87%E5%90%8E%E7%BC%80%E5%90%8D%E5%88%A4%E6%96%AD%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E6%98%AF%E5%90%A6%E6%BB%A1%E8%B6%B3%E8%A6%81%E6%B1%82%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20if%20filename.endswith(%22.md%22)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20%E5%A6%82%E6%9E%9C%E6%BB%A1%E8%B6%B3%E8%A6%81%E6%B1%82%EF%BC%8C%E5%B0%86%E5%85%B6%E7%BB%9D%E5%AF%B9%E8%B7%AF%E5%BE%84%E5%8A%A0%E5%85%A5%E5%88%B0%E7%BB%93%E6%9E%9C%E5%88%97%E8%A1%A8%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20file_list.append(os.path.join(filepath%2C%20filename))%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20elif%20filename.endswith(%22.txt%22)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20file_list.append(os.path.join(filepath%2C%20filename))%0A%C2%A0%20%C2%A0%20return%20file_list%0A%60%60%60%0A%23%23%23%202.2%20%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%0A%E5%BE%97%E5%88%B0%E6%89%80%E6%9C%89%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%20LangChain%20%E6%8F%90%E4%BE%9B%E7%9A%84%20FileLoader%20%E5%AF%B9%E8%B1%A1%E6%9D%A5%E5%8A%A0%E8%BD%BD%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%EF%BC%8C%E5%BE%97%E5%88%B0%E7%94%B1%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90%E5%87%BA%E7%9A%84%E7%BA%AF%E6%96%87%E6%9C%AC%E5%86%85%E5%AE%B9%E3%80%82%E7%94%B1%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%96%87%E4%BB%B6%E9%9C%80%E8%A6%81%E5%AF%B9%E5%BA%94%E4%B8%8D%E5%90%8C%E7%9A%84%20FileLoader%EF%BC%8C%E6%88%91%E4%BB%AC%E5%88%A4%E6%96%AD%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%EF%BC%8C%E5%B9%B6%E9%92%88%E5%AF%B9%E6%80%A7%E8%B0%83%E7%94%A8%E5%AF%B9%E5%BA%94%E7%B1%BB%E5%9E%8B%E7%9A%84%20FileLoader%EF%BC%8C%E5%90%8C%E6%97%B6%EF%BC%8C%E8%B0%83%E7%94%A8%20FileLoader%20%E5%AF%B9%E8%B1%A1%E7%9A%84%20load%20%E6%96%B9%E6%B3%95%E6%9D%A5%E5%BE%97%E5%88%B0%E5%8A%A0%E8%BD%BD%E4%B9%8B%E5%90%8E%E7%9A%84%E7%BA%AF%E6%96%87%E6%9C%AC%E5%AF%B9%E8%B1%A1%EF%BC%9A%0A%60%60%60python%0Afrom%20tqdm%20import%20tqdm%0Afrom%20langchain.document_loaders%20import%20UnstructuredFileLoader%0Afrom%20langchain.document_loaders%20import%20UnstructuredMarkdownLoader%0Adef%20get_text(dir_path)%3A%0A%C2%A0%20%C2%A0%20%23%20args%EF%BC%9Adir_path%EF%BC%8C%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%B7%AF%E5%BE%84%0A%C2%A0%20%C2%A0%20%23%20%E9%A6%96%E5%85%88%E8%B0%83%E7%94%A8%E4%B8%8A%E6%96%87%E5%AE%9A%E4%B9%89%E7%9A%84%E5%87%BD%E6%95%B0%E5%BE%97%E5%88%B0%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84%E5%88%97%E8%A1%A8%0A%C2%A0%20%C2%A0%20file_lst%20%3D%20get_files(dir_path)%0A%C2%A0%20%C2%A0%20%23%20docs%20%E5%AD%98%E6%94%BE%E5%8A%A0%E8%BD%BD%E4%B9%8B%E5%90%8E%E7%9A%84%E7%BA%AF%E6%96%87%E6%9C%AC%E5%AF%B9%E8%B1%A1%0A%C2%A0%20%C2%A0%20docs%20%3D%20%5B%5D%0A%C2%A0%20%C2%A0%20%23%20%E9%81%8D%E5%8E%86%E6%89%80%E6%9C%89%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%0A%C2%A0%20%C2%A0%20for%20one_file%20in%20tqdm(file_lst)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20file_type%20%3D%20one_file.split('.')%5B-1%5D%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20if%20file_type%20%3D%3D%20'md'%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20loader%20%3D%20UnstructuredMarkdownLoader(one_file)%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20elif%20file_type%20%3D%3D%20'txt'%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20loader%20%3D%20UnstructuredFileLoader(one_file)%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20else%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20%E5%A6%82%E6%9E%9C%E6%98%AF%E4%B8%8D%E7%AC%A6%E5%90%88%E6%9D%A1%E4%BB%B6%E7%9A%84%E6%96%87%E4%BB%B6%EF%BC%8C%E7%9B%B4%E6%8E%A5%E8%B7%B3%E8%BF%87%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20continue%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20docs.extend(loader.load())%0A%C2%A0%20%C2%A0%20return%20docs%0A%60%60%60%0A%E4%BD%BF%E7%94%A8%E4%B8%8A%E6%96%87%E5%87%BD%E6%95%B0%EF%BC%8C%E6%88%91%E4%BB%AC%E5%BE%97%E5%88%B0%E7%9A%84%20%60docs%60%20%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%BA%AF%E6%96%87%E6%9C%AC%E5%AF%B9%E8%B1%A1%E5%AF%B9%E5%BA%94%E7%9A%84%E5%88%97%E8%A1%A8%E3%80%82%0A%23%23%23%202.3%20%E6%9E%84%E5%BB%BA%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%0A%E5%BE%97%E5%88%B0%E8%AF%A5%E5%88%97%E8%A1%A8%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E5%8F%AF%E4%BB%A5%E5%B0%86%E5%AE%83%E5%BC%95%E5%85%A5%E5%88%B0%20LangChain%20%E6%A1%86%E6%9E%B6%E4%B8%AD%E6%9E%84%E5%BB%BA%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%82%E7%94%B1%E7%BA%AF%E6%96%87%E6%9C%AC%E5%AF%B9%E8%B1%A1%E6%9E%84%E5%BB%BA%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%8C%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%85%88%E5%AF%B9%E6%96%87%E6%9C%AC%E8%BF%9B%E8%A1%8C%E5%88%86%E5%9D%97%EF%BC%8C%E6%8E%A5%E7%9D%80%E5%AF%B9%E6%96%87%E6%9C%AC%E5%9D%97%E8%BF%9B%E8%A1%8C%E5%90%91%E9%87%8F%E5%8C%96%E3%80%82%0ALangChain%20%E6%8F%90%E4%BE%9B%E4%BA%86%E5%A4%9A%E7%A7%8D%E6%96%87%E6%9C%AC%E5%88%86%E5%9D%97%E5%B7%A5%E5%85%B7%EF%BC%8C%E6%AD%A4%E5%A4%84%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%E5%AD%97%E7%AC%A6%E4%B8%B2%E9%80%92%E5%BD%92%E5%88%86%E5%89%B2%E5%99%A8%EF%BC%8C%E5%B9%B6%E9%80%89%E6%8B%A9%E5%88%86%E5%9D%97%E5%A4%A7%E5%B0%8F%E4%B8%BA%20500%EF%BC%8C%E5%9D%97%E9%87%8D%E5%8F%A0%E9%95%BF%E5%BA%A6%E4%B8%BA%20150%0A%60%60%60python%0Afrom%20langchain.text_splitter%20import%20RecursiveCharacterTextSplitter%0Atext_splitter%20%3D%20RecursiveCharacterTextSplitter(%0A%C2%A0%20%C2%A0%20chunk_size%3D500%2C%20chunk_overlap%3D150)%0Asplit_docs%20%3D%20text_splitter.split_documents(docs)%0A%60%60%60%0A%E6%8E%A5%E7%9D%80%E6%88%91%E4%BB%AC%E9%80%89%E7%94%A8%E5%BC%80%E6%BA%90%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%20%5BSentence%20Transformer%5D(https%3A%2F%2Fhuggingface.co%2Fsentence-transformers%2Fparaphrase-multilingual-MiniLM-L12-v2)%20%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96%E3%80%82LangChain%20%E6%8F%90%E4%BE%9B%E4%BA%86%E7%9B%B4%E6%8E%A5%E5%BC%95%E5%85%A5%20HuggingFace%20%E5%BC%80%E6%BA%90%E7%A4%BE%E5%8C%BA%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%90%91%E9%87%8F%E5%8C%96%E7%9A%84%E6%8E%A5%E5%8F%A3%EF%BC%9A%0A%60%60%60python%0Afrom%20langchain.embeddings.huggingface%20import%20HuggingFaceEmbeddings%0Aembeddings%20%3D%20HuggingFaceEmbeddings(model_name%3D%22%2Froot%2Fdata%2Fmodel%2Fsentence-transformer%22)%0A%60%60%60%0A%E5%90%8C%E6%97%B6%EF%BC%8C%E8%80%83%E8%99%91%E5%88%B0%20Chroma%20%E6%98%AF%E7%9B%AE%E5%89%8D%E6%9C%80%E5%B8%B8%E7%94%A8%E7%9A%84%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%8C%E6%88%91%E4%BB%AC%E9%80%89%E6%8B%A9%20Chroma%20%E4%BD%9C%E4%B8%BA%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%8C%E5%9F%BA%E4%BA%8E%E4%B8%8A%E6%96%87%E5%88%86%E5%9D%97%E5%90%8E%E7%9A%84%E6%96%87%E6%A1%A3%E4%BB%A5%E5%8F%8A%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%BC%80%E6%BA%90%E5%90%91%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B0%86%E8%AF%AD%E6%96%99%E5%8A%A0%E8%BD%BD%E5%88%B0%E6%8C%87%E5%AE%9A%E8%B7%AF%E5%BE%84%E4%B8%8B%E7%9A%84%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9A%0A%60%60%60python%0Afrom%20langchain.vectorstores%20import%20Chroma%0A%23%20%E5%AE%9A%E4%B9%89%E6%8C%81%E4%B9%85%E5%8C%96%E8%B7%AF%E5%BE%84%0Apersist_directory%20%3D%20'data_base%2Fvector_db%2Fchroma'%0A%23%20%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%BA%93%0Avectordb%20%3D%20Chroma.from_documents(%0A%C2%A0%20%C2%A0%20documents%3Dsplit_docs%2C%0A%C2%A0%20%C2%A0%20embedding%3Dembeddings%2C%0A%C2%A0%20%C2%A0%20persist_directory%3Dpersist_directory%20%C2%A0%23%20%E5%85%81%E8%AE%B8%E6%88%91%E4%BB%AC%E5%B0%86persist_directory%E7%9B%AE%E5%BD%95%E4%BF%9D%E5%AD%98%E5%88%B0%E7%A3%81%E7%9B%98%E4%B8%8A%0A)%0A%23%20%E5%B0%86%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8C%81%E4%B9%85%E5%8C%96%E5%88%B0%E7%A3%81%E7%9B%98%E4%B8%8A%0Avectordb.persist()%0A%60%60%60%0A%0A%23%23%203%20InternLM%20%E6%8E%A5%E5%85%A5%20LangChain%0A%E4%B8%BA%E4%BE%BF%E6%8D%B7%E6%9E%84%E5%BB%BA%20LLM%20%E5%BA%94%E7%94%A8%EF%BC%8C%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%9F%BA%E4%BA%8E%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E7%9A%84%20InternLM%EF%BC%8C%E7%BB%A7%E6%89%BF%20LangChain%20%E7%9A%84%20LLM%20%E7%B1%BB%E8%87%AA%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%20InternLM%20LLM%20%E5%AD%90%E7%B1%BB%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E5%B0%86%20InternLM%20%E6%8E%A5%E5%85%A5%E5%88%B0%20LangChain%20%E6%A1%86%E6%9E%B6%E4%B8%AD%E3%80%82%E5%AE%8C%E6%88%90%20LangChain%20%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%20LLM%20%E5%AD%90%E7%B1%BB%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BB%A5%E5%AE%8C%E5%85%A8%E4%B8%80%E8%87%B4%E7%9A%84%E6%96%B9%E5%BC%8F%E8%B0%83%E7%94%A8%20LangChain%20%E7%9A%84%E6%8E%A5%E5%8F%A3%EF%BC%8C%E8%80%8C%E6%97%A0%E9%9C%80%E8%80%83%E8%99%91%E5%BA%95%E5%B1%82%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E7%9A%84%E4%B8%8D%E4%B8%80%E8%87%B4%E3%80%82%0A%E5%9F%BA%E4%BA%8E%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E7%9A%84%20InternLM%20%E8%87%AA%E5%AE%9A%E4%B9%89%20LLM%20%E7%B1%BB%E5%B9%B6%E4%B8%8D%E5%A4%8D%E6%9D%82%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AA%E9%9C%80%E4%BB%8E%20LangChain.llms.base.LLM%20%E7%B1%BB%E7%BB%A7%E6%89%BF%E4%B8%80%E4%B8%AA%E5%AD%90%E7%B1%BB%EF%BC%8C%E5%B9%B6%E9%87%8D%E5%86%99%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E4%B8%8E%20%60_call%60%20%E5%87%BD%E6%95%B0%E5%8D%B3%E5%8F%AF%EF%BC%9A%0A%60%60%60python%0Afrom%20langchain.llms.base%20import%20LLM%0Afrom%20typing%20import%20Any%2C%20List%2C%20Optional%0Afrom%20langchain.callbacks.manager%20import%20CallbackManagerForLLMRun%0Afrom%20transformers%20import%20AutoTokenizer%2C%20AutoModelForCausalLM%0Aimport%20torch%0Aclass%20InternLM_LLM(LLM)%3A%0A%C2%A0%20%C2%A0%20%23%20%E5%9F%BA%E4%BA%8E%E6%9C%AC%E5%9C%B0%20InternLM%20%E8%87%AA%E5%AE%9A%E4%B9%89%20LLM%20%E7%B1%BB%0A%C2%A0%20%C2%A0%20tokenizer%20%3A%20AutoTokenizer%20%3D%20None%0A%C2%A0%20%C2%A0%20model%3A%20AutoModelForCausalLM%20%3D%20None%0A%C2%A0%20%C2%A0%20def%20__init__(self%2C%20model_path%20%3Astr)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20model_path%3A%20InternLM%20%E6%A8%A1%E5%9E%8B%E8%B7%AF%E5%BE%84%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20super().__init__()%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20print(%22%E6%AD%A3%E5%9C%A8%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B...%22)%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20self.tokenizer%20%3D%20AutoTokenizer.from_pretrained(model_path%2C%20trust_remote_code%3DTrue)%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20self.model%20%3D%20AutoModelForCausalLM.from_pretrained(model_path%2C%20trust_remote_code%3DTrue).to(torch.bfloat16).cuda()%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20self.model%20%3D%20self.model.eval()%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20print(%22%E5%AE%8C%E6%88%90%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A0%E8%BD%BD%22)%0A%C2%A0%20%C2%A0%20def%20_call(self%2C%20prompt%20%3A%20str%2C%20stop%3A%20Optional%5BList%5Bstr%5D%5D%20%3D%20None%2C%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20run_manager%3A%20Optional%5BCallbackManagerForLLMRun%5D%20%3D%20None%2C%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20**kwargs%3A%20Any)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20%E9%87%8D%E5%86%99%E8%B0%83%E7%94%A8%E5%87%BD%E6%95%B0%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20system_prompt%20%3D%20%22%22%22You%20are%20an%20AI%20assistant%20whose%20name%20is%20InternLM%20(%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD).%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20-%20InternLM%20(%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD)%20is%20a%20conversational%20language%20model%20that%20is%20developed%20by%20Shanghai%20AI%20Laboratory%20(%E4%B8%8A%E6%B5%B7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9E%E9%AA%8C%E5%AE%A4).%20It%20is%20designed%20to%20be%20helpful%2C%20honest%2C%20and%20harmless.%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20-%20InternLM%20(%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD)%20can%20understand%20and%20communicate%20fluently%20in%20the%20language%20chosen%20by%20the%20user%20such%20as%20English%20and%20%E4%B8%AD%E6%96%87.%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%22%22%22%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20messages%20%3D%20%5B(system_prompt%2C%20'')%5D%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20response%2C%20history%20%3D%20self.model.chat(self.tokenizer%2C%20prompt%20%2C%20history%3Dmessages)%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20return%20response%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%0A%C2%A0%20%C2%A0%20%40property%0A%C2%A0%20%C2%A0%20def%20_llm_type(self)%20-%3E%20str%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20return%20%22InternLM%22%0A%60%60%60%0A%E5%9C%A8%E4%B8%8A%E8%BF%B0%E7%B1%BB%E5%AE%9A%E4%B9%89%E4%B8%AD%EF%BC%8C%E6%88%91%E4%BB%AC%E5%88%86%E5%88%AB%E9%87%8D%E5%86%99%E4%BA%86%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E5%92%8C%20%60_call%60%20%E5%87%BD%E6%95%B0%EF%BC%9A%E5%AF%B9%E4%BA%8E%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%EF%BC%8C%E6%88%91%E4%BB%AC%E5%9C%A8%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E7%9A%84%E4%B8%80%E5%BC%80%E5%A7%8B%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E7%9A%84%20InternLM%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%8E%E8%80%8C%E9%81%BF%E5%85%8D%E6%AF%8F%E4%B8%80%E6%AC%A1%E8%B0%83%E7%94%A8%E9%83%BD%E9%9C%80%E8%A6%81%E9%87%8D%E6%96%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%97%B6%E9%97%B4%E8%BF%87%E9%95%BF%EF%BC%9B%60_call%60%20%E5%87%BD%E6%95%B0%E6%98%AF%20LLM%20%E7%B1%BB%E7%9A%84%E6%A0%B8%E5%BF%83%E5%87%BD%E6%95%B0%EF%BC%8CLangChain%20%E4%BC%9A%E8%B0%83%E7%94%A8%E8%AF%A5%E5%87%BD%E6%95%B0%E6%9D%A5%E8%B0%83%E7%94%A8%20LLM%EF%BC%8C%E5%9C%A8%E8%AF%A5%E5%87%BD%E6%95%B0%E4%B8%AD%EF%BC%8C%E6%88%91%E4%BB%AC%E8%B0%83%E7%94%A8%E5%B7%B2%E5%AE%9E%E4%BE%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E7%9A%84%20chat%20%E6%96%B9%E6%B3%95%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E5%AF%B9%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%B0%83%E7%94%A8%E5%B9%B6%E8%BF%94%E5%9B%9E%E8%B0%83%E7%94%A8%E7%BB%93%E6%9E%9C%E3%80%82%0A%E5%9C%A8%E6%95%B4%E4%BD%93%E9%A1%B9%E7%9B%AE%E4%B8%AD%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%86%E4%B8%8A%E8%BF%B0%E4%BB%A3%E7%A0%81%E5%B0%81%E8%A3%85%E4%B8%BA%20LLM.py%EF%BC%8C%E5%90%8E%E7%BB%AD%E5%B0%86%E7%9B%B4%E6%8E%A5%E4%BB%8E%E8%AF%A5%E6%96%87%E4%BB%B6%E4%B8%AD%E5%BC%95%E5%85%A5%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%20LLM%20%E7%B1%BB%E3%80%82%0A%23%23%204%20%E6%9E%84%E5%BB%BA%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%0A%E8%B0%83%E7%94%A8%E4%B8%80%E4%B8%AA%20LangChain%20%E6%8F%90%E4%BE%9B%E7%9A%84%20%60RetrievalQA%60%20%E5%AF%B9%E8%B1%A1%EF%BC%8C%E9%80%9A%E8%BF%87%E5%88%9D%E5%A7%8B%E5%8C%96%E6%97%B6%E5%A1%AB%E5%85%A5%E5%B7%B2%E6%9E%84%E5%BB%BA%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E8%87%AA%E5%AE%9A%E4%B9%89%20LLM%20%E4%BD%9C%E4%B8%BA%E5%8F%82%E6%95%B0%EF%BC%8C%E6%9D%A5%E7%AE%80%E4%BE%BF%E5%9C%B0%E5%AE%8C%E6%88%90%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E9%97%AE%E7%AD%94%E7%9A%84%E5%85%A8%E6%B5%81%E7%A8%8B%EF%BC%8CLangChain%20%E4%BC%9A%E8%87%AA%E5%8A%A8%E5%AE%8C%E6%88%90%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E6%8F%90%E9%97%AE%E8%BF%9B%E8%A1%8C%E6%A3%80%E7%B4%A2%E3%80%81%E8%8E%B7%E5%8F%96%E7%9B%B8%E5%85%B3%E6%96%87%E6%A1%A3%E3%80%81%E6%8B%BC%E6%8E%A5%E4%B8%BA%E5%90%88%E9%80%82%E7%9A%84%20Prompt%20%E5%B9%B6%E4%BA%A4%E7%BB%99%20LLM%20%E9%97%AE%E7%AD%94%E7%9A%84%E5%85%A8%E9%83%A8%E6%B5%81%E7%A8%8B%E3%80%82%0A%23%23%23%204.1%20%E5%8A%A0%E8%BD%BD%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%0A%E9%A6%96%E5%85%88%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%B0%86%E4%B8%8A%E6%96%87%E6%9E%84%E5%BB%BA%E7%9A%84%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%BC%E5%85%A5%E8%BF%9B%E6%9D%A5%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E9%80%9A%E8%BF%87%20Chroma%20%E4%BB%A5%E5%8F%8A%E4%B8%8A%E6%96%87%E5%AE%9A%E4%B9%89%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%8A%A0%E8%BD%BD%E5%B7%B2%E6%9E%84%E5%BB%BA%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9A%0A%60%60%60python%0Afrom%20langchain.vectorstores%20import%20Chroma%0Afrom%20langchain.embeddings.huggingface%20import%20HuggingFaceEmbeddings%0Aimport%20os%0A%23%20%E5%AE%9A%E4%B9%89%20Embeddings%0Aembeddings%20%3D%20HuggingFaceEmbeddings(model_name%3D%22%2Froot%2Fdata%2Fmodel%2Fsentence-transformer%22)%0A%23%20%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8C%81%E4%B9%85%E5%8C%96%E8%B7%AF%E5%BE%84%0Apersist_directory%20%3D%20'data_base%2Fvector_db%2Fchroma'%0A%23%20%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%BA%93%0Avectordb%20%3D%20Chroma(%0A%C2%A0%20%C2%A0%20persist_directory%3Dpersist_directory%2C%0A%C2%A0%20%C2%A0%20embedding_function%3Dembeddings%0A)%0A%60%60%60%0A%E4%B8%8A%E8%BF%B0%E4%BB%A3%E7%A0%81%E5%BE%97%E5%88%B0%E7%9A%84%20%60vectordb%60%20%E5%AF%B9%E8%B1%A1%E5%8D%B3%E4%B8%BA%E6%88%91%E4%BB%AC%E5%B7%B2%E6%9E%84%E5%BB%BA%E7%9A%84%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E8%B1%A1%EF%BC%8C%E8%AF%A5%E5%AF%B9%E8%B1%A1%E5%8F%AF%E4%BB%A5%E9%92%88%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%20%60query%60%20%E8%BF%9B%E8%A1%8C%E8%AF%AD%E4%B9%89%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%EF%BC%8C%E5%BE%97%E5%88%B0%E4%B8%8E%E7%94%A8%E6%88%B7%E6%8F%90%E9%97%AE%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%89%87%E6%AE%B5%E3%80%82%0A%23%23%23%204.2%20%E5%AE%9E%E4%BE%8B%E5%8C%96%E8%87%AA%E5%AE%9A%E4%B9%89%20LLM%20%E4%B8%8E%20Prompt%20Template%0A%E6%8E%A5%E7%9D%80%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AE%9E%E4%BE%8B%E5%8C%96%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8E%20InternLM%20%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%20LLM%20%E5%AF%B9%E8%B1%A1%EF%BC%9A%0A%60%60%60python%0Afrom%20LLM%20import%20InternLM_LLM%0Allm%20%3D%20InternLM_LLM(model_path%20%3D%20%22%2Froot%2Fdata%2Fmodel%2FShanghai_AI_Laboratory%2Finternlm-chat-7b%22)%0Allm.predict(%22%E4%BD%A0%E6%98%AF%E8%B0%81%22)%0A%60%60%60%0A%E6%9E%84%E5%BB%BA%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%EF%BC%8C%E8%BF%98%E9%9C%80%E8%A6%81%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%20Prompt%20Template%EF%BC%8C%E8%AF%A5%20Template%20%E5%85%B6%E5%AE%9E%E5%9F%BA%E4%BA%8E%E4%B8%80%E4%B8%AA%E5%B8%A6%E5%8F%98%E9%87%8F%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%EF%BC%8C%E5%9C%A8%E6%A3%80%E7%B4%A2%E4%B9%8B%E5%90%8E%EF%BC%8CLangChain%20%E4%BC%9A%E5%B0%86%E6%A3%80%E7%B4%A2%E5%88%B0%E7%9A%84%E7%9B%B8%E5%85%B3%E6%96%87%E6%A1%A3%E7%89%87%E6%AE%B5%E5%A1%AB%E5%85%A5%E5%88%B0%20Template%20%E7%9A%84%E5%8F%98%E9%87%8F%E4%B8%AD%EF%BC%8C%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E5%B8%A6%E7%9F%A5%E8%AF%86%E7%9A%84%20Prompt%20%E6%9E%84%E5%BB%BA%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%9F%BA%E4%BA%8E%20LangChain%20%E7%9A%84%20Template%20%E5%9F%BA%E7%B1%BB%E6%9D%A5%E5%AE%9E%E4%BE%8B%E5%8C%96%E8%BF%99%E6%A0%B7%E4%B8%80%E4%B8%AA%20Template%20%E5%AF%B9%E8%B1%A1%EF%BC%9A%0A%60%60%60python%0Afrom%20langchain.prompts%20import%20PromptTemplate%0A%23%20%E6%88%91%E4%BB%AC%E6%89%80%E6%9E%84%E9%80%A0%E7%9A%84%20Prompt%20%E6%A8%A1%E6%9D%BF%0Atemplate%20%3D%20%22%22%22%E4%BD%BF%E7%94%A8%E4%BB%A5%E4%B8%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E6%9D%A5%E5%9B%9E%E7%AD%94%E7%94%A8%E6%88%B7%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82%E5%A6%82%E6%9E%9C%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%AD%94%E6%A1%88%EF%BC%8C%E5%B0%B1%E8%AF%B4%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E3%80%82%E6%80%BB%E6%98%AF%E4%BD%BF%E7%94%A8%E4%B8%AD%E6%96%87%E5%9B%9E%E7%AD%94%E3%80%82%0A%E9%97%AE%E9%A2%98%3A%20%7Bquestion%7D%0A%E5%8F%AF%E5%8F%82%E8%80%83%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%EF%BC%9A%0A%C2%B7%C2%B7%C2%B7%0A%7Bcontext%7D%0A%C2%B7%C2%B7%C2%B7%0A%E5%A6%82%E6%9E%9C%E7%BB%99%E5%AE%9A%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E6%B3%95%E8%AE%A9%E4%BD%A0%E5%81%9A%E5%87%BA%E5%9B%9E%E7%AD%94%EF%BC%8C%E8%AF%B7%E5%9B%9E%E7%AD%94%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E3%80%82%0A%E6%9C%89%E7%94%A8%E7%9A%84%E5%9B%9E%E7%AD%94%3A%22%22%22%0A%23%20%E8%B0%83%E7%94%A8%20LangChain%20%E7%9A%84%E6%96%B9%E6%B3%95%E6%9D%A5%E5%AE%9E%E4%BE%8B%E5%8C%96%E4%B8%80%E4%B8%AA%20Template%20%E5%AF%B9%E8%B1%A1%EF%BC%8C%E8%AF%A5%E5%AF%B9%E8%B1%A1%E5%8C%85%E5%90%AB%E4%BA%86%20context%20%E5%92%8C%20question%20%E4%B8%A4%E4%B8%AA%E5%8F%98%E9%87%8F%EF%BC%8C%E5%9C%A8%E5%AE%9E%E9%99%85%E8%B0%83%E7%94%A8%E6%97%B6%EF%BC%8C%E8%BF%99%E4%B8%A4%E4%B8%AA%E5%8F%98%E9%87%8F%E4%BC%9A%E8%A2%AB%E6%A3%80%E7%B4%A2%E5%88%B0%E7%9A%84%E6%96%87%E6%A1%A3%E7%89%87%E6%AE%B5%E5%92%8C%E7%94%A8%E6%88%B7%E6%8F%90%E9%97%AE%E5%A1%AB%E5%85%85%0AQA_CHAIN_PROMPT%20%3D%20PromptTemplate(input_variables%3D%5B%22context%22%2C%22question%22%5D%2Ctemplate%3Dtemplate)%0A%60%60%60%0A%23%23%23%204.3%20%E6%9E%84%E5%BB%BA%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%0A%E6%9C%80%E5%90%8E%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%B0%83%E7%94%A8%20LangChain%20%E6%8F%90%E4%BE%9B%E7%9A%84%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%EF%BC%8C%E5%9F%BA%E4%BA%8E%E6%88%91%E4%BB%AC%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%20LLM%E3%80%81Prompt%20Template%20%E5%92%8C%E5%90%91%E9%87%8F%E7%9F%A5%E8%AF%86%E5%BA%93%E6%9D%A5%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8E%20InternLM%20%E7%9A%84%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%EF%BC%9A%0A%60%60%60python%0Afrom%20langchain.chains%20import%20RetrievalQA%0Aqa_chain%20%3D%20RetrievalQA.from_chain_type(llm%2Cretriever%3Dvectordb.as_retriever()%2Creturn_source_documents%3DTrue%2Cchain_type_kwargs%3D%7B%22prompt%22%3AQA_CHAIN_PROMPT%7D)%0A%60%60%60%0A%E5%BE%97%E5%88%B0%E7%9A%84%20%60qa_chain%60%20%E5%AF%B9%E8%B1%A1%E5%8D%B3%E5%8F%AF%E4%BB%A5%E5%AE%9E%E7%8E%B0%E6%88%91%E4%BB%AC%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%EF%BC%8C%E5%8D%B3%E5%9F%BA%E4%BA%8E%20InternLM%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%AF%B9%E6%AF%94%E8%AF%A5%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E5%92%8C%E7%BA%AF%20LLM%20%E7%9A%84%E9%97%AE%E7%AD%94%E6%95%88%E6%9E%9C%EF%BC%9A%0A%60%60%60python%0A%23%20%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E5%9B%9E%E7%AD%94%E6%95%88%E6%9E%9C%0Aquestion%20%3D%20%22%E4%BB%80%E4%B9%88%E6%98%AFInternLM%22%0Aresult%20%3D%20qa_chain(%7B%22query%22%3A%20question%7D)%0Aprint(%22%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E5%9B%9E%E7%AD%94%20question%20%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%9A%22)%0Aprint(result%5B%22result%22%5D)%0A%23%20%E4%BB%85%20LLM%20%E5%9B%9E%E7%AD%94%E6%95%88%E6%9E%9C%0Aresult_2%20%3D%20llm(question)%0Aprint(%22%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%9E%E7%AD%94%20question%20%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%9A%22)%0Aprint(result_2)%0A%60%60%60%0A%23%23%205%20%E9%83%A8%E7%BD%B2%20Web%20Demo%0A%E5%9C%A8%E5%AE%8C%E6%88%90%E4%B8%8A%E8%BF%B0%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E5%90%8E%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%9F%BA%E4%BA%8E%20Gradio%20%E6%A1%86%E6%9E%B6%E5%B0%86%E5%85%B6%E9%83%A8%E7%BD%B2%E5%88%B0%20Web%20%E7%BD%91%E9%A1%B5%EF%BC%8C%E4%BB%8E%E8%80%8C%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%9E%8B%20Demo%EF%BC%8C%E4%BE%BF%E4%BA%8E%E6%B5%8B%E8%AF%95%E4%B8%8E%E4%BD%BF%E7%94%A8%E3%80%82%0A%E6%88%91%E4%BB%AC%E9%A6%96%E5%85%88%E5%B0%86%E4%B8%8A%E6%96%87%E7%9A%84%E4%BB%A3%E7%A0%81%E5%86%85%E5%AE%B9%E5%B0%81%E8%A3%85%E4%B8%BA%E4%B8%80%E4%B8%AA%E8%BF%94%E5%9B%9E%E6%9E%84%E5%BB%BA%E7%9A%84%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%87%BD%E6%95%B0%EF%BC%8C%E5%B9%B6%E5%9C%A8%E5%90%AF%E5%8A%A8%20Gradio%20%E7%9A%84%E7%AC%AC%E4%B8%80%E6%97%B6%E9%97%B4%E8%B0%83%E7%94%A8%E8%AF%A5%E5%87%BD%E6%95%B0%E5%BE%97%E5%88%B0%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E5%AF%B9%E8%B1%A1%EF%BC%8C%E5%90%8E%E7%BB%AD%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E8%AF%A5%E5%AF%B9%E8%B1%A1%E8%BF%9B%E8%A1%8C%E9%97%AE%E7%AD%94%E5%AF%B9%E8%AF%9D%EF%BC%8C%E4%BB%8E%E8%80%8C%E9%81%BF%E5%85%8D%E9%87%8D%E5%A4%8D%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%EF%BC%9A%0A%60%60%60python%0Afrom%20langchain.vectorstores%20import%20Chroma%0Afrom%20langchain.embeddings.huggingface%20import%20HuggingFaceEmbeddings%0Aimport%20os%0Afrom%20LLM%20import%20InternLM_LLM%0Afrom%20langchain.prompts%20import%20PromptTemplate%0Afrom%20langchain.chains%20import%20RetrievalQA%0Adef%20load_chain()%3A%0A%C2%A0%20%C2%A0%20%23%20%E5%8A%A0%E8%BD%BD%E9%97%AE%E7%AD%94%E9%93%BE%0A%C2%A0%20%C2%A0%20%23%20%E5%AE%9A%E4%B9%89%20Embeddings%0A%C2%A0%20%C2%A0%20embeddings%20%3D%20HuggingFaceEmbeddings(model_name%3D%22%2Froot%2Fdata%2Fmodel%2Fsentence-transformer%22)%0A%C2%A0%20%C2%A0%20%23%20%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8C%81%E4%B9%85%E5%8C%96%E8%B7%AF%E5%BE%84%0A%C2%A0%20%C2%A0%20persist_directory%20%3D%20'data_base%2Fvector_db%2Fchroma'%0A%C2%A0%20%C2%A0%20%23%20%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%BA%93%0A%C2%A0%20%C2%A0%20vectordb%20%3D%20Chroma(%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20persist_directory%3Dpersist_directory%2C%20%C2%A0%23%20%E5%85%81%E8%AE%B8%E6%88%91%E4%BB%AC%E5%B0%86persist_directory%E7%9B%AE%E5%BD%95%E4%BF%9D%E5%AD%98%E5%88%B0%E7%A3%81%E7%9B%98%E4%B8%8A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20embedding_function%3Dembeddings%0A%C2%A0%20%C2%A0%20)%0A%C2%A0%20%C2%A0%20%23%20%E5%8A%A0%E8%BD%BD%E8%87%AA%E5%AE%9A%E4%B9%89%20LLM%0A%C2%A0%20%C2%A0%20llm%20%3D%20InternLM_LLM(model_path%20%3D%20%22%2Froot%2Fdata%2Fmodel%2FShanghai_AI_Laboratory%2Finternlm-chat-7b%22)%0A%C2%A0%20%C2%A0%20%23%20%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%20Prompt%20Template%0A%C2%A0%20%C2%A0%20template%20%3D%20%22%22%22%E4%BD%BF%E7%94%A8%E4%BB%A5%E4%B8%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E6%9D%A5%E5%9B%9E%E7%AD%94%E6%9C%80%E5%90%8E%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82%E5%A6%82%E6%9E%9C%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%E7%AD%94%E6%A1%88%EF%BC%8C%E5%B0%B1%E8%AF%B4%E4%BD%A0%E4%B8%8D%E7%9F%A5%E9%81%93%EF%BC%8C%E4%B8%8D%E8%A6%81%E8%AF%95%E5%9B%BE%E7%BC%96%E9%80%A0%E7%AD%94%0A%C2%A0%20%C2%A0%20%E6%A1%88%E3%80%82%E5%B0%BD%E9%87%8F%E4%BD%BF%E7%AD%94%E6%A1%88%E7%AE%80%E6%98%8E%E6%89%BC%E8%A6%81%E3%80%82%E6%80%BB%E6%98%AF%E5%9C%A8%E5%9B%9E%E7%AD%94%E7%9A%84%E6%9C%80%E5%90%8E%E8%AF%B4%E2%80%9C%E8%B0%A2%E8%B0%A2%E4%BD%A0%E7%9A%84%E6%8F%90%E9%97%AE%EF%BC%81%E2%80%9D%E3%80%82%0A%C2%A0%20%C2%A0%20%7Bcontext%7D%0A%C2%A0%20%C2%A0%20%E9%97%AE%E9%A2%98%3A%20%7Bquestion%7D%0A%C2%A0%20%C2%A0%20%E6%9C%89%E7%94%A8%E7%9A%84%E5%9B%9E%E7%AD%94%3A%22%22%22%0A%C2%A0%20%C2%A0%20QA_CHAIN_PROMPT%20%3D%20PromptTemplate(input_variables%3D%5B%22context%22%2C%22question%22%5D%2Ctemplate%3Dtemplate)%0A%C2%A0%20%C2%A0%20%23%20%E8%BF%90%E8%A1%8C%20chain%0A%C2%A0%20%C2%A0%20qa_chain%20%3D%20RetrievalQA.from_chain_type(llm%2Cretriever%3Dvectordb.as_retriever()%2Creturn_source_documents%3DTrue%2Cchain_type_kwargs%3D%7B%22prompt%22%3AQA_CHAIN_PROMPT%7D)%0A%C2%A0%20%C2%A0%0A%C2%A0%20%C2%A0%20return%20qa_chain%0A%60%60%60%0A%E6%8E%A5%E7%9D%80%E6%88%91%E4%BB%AC%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E7%B1%BB%EF%BC%8C%E8%AF%A5%E7%B1%BB%E8%B4%9F%E8%B4%A3%E5%8A%A0%E8%BD%BD%E5%B9%B6%E5%AD%98%E5%82%A8%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%EF%BC%8C%E5%B9%B6%E5%93%8D%E5%BA%94%20Web%20%E7%95%8C%E9%9D%A2%E9%87%8C%E8%B0%83%E7%94%A8%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E8%BF%9B%E8%A1%8C%E5%9B%9E%E7%AD%94%E7%9A%84%E5%8A%A8%E4%BD%9C%EF%BC%9A%0A%60%60%60python%0Aclass%20Model_center()%3A%0A%C2%A0%20%C2%A0%20%22%22%22%0A%C2%A0%20%C2%A0%20%E5%AD%98%E5%82%A8%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%E7%9A%84%E5%AF%B9%E8%B1%A1%0A%C2%A0%20%C2%A0%20%22%22%22%0A%C2%A0%20%C2%A0%20def%20__init__(self)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%EF%BC%8C%E5%8A%A0%E8%BD%BD%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20self.chain%20%3D%20load_chain()%0A%C2%A0%20%C2%A0%20def%20qa_chain_self_answer(self%2C%20question%3A%20str%2C%20chat_history%3A%20list%20%3D%20%5B%5D)%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%22%22%22%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%E8%B0%83%E7%94%A8%E9%97%AE%E7%AD%94%E9%93%BE%E8%BF%9B%E8%A1%8C%E5%9B%9E%E7%AD%94%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%22%22%22%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20if%20question%20%3D%3D%20None%20or%20len(question)%20%3C%201%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20return%20%22%22%2C%20chat_history%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20try%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20chat_history.append(%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20(question%2C%20self.chain(%7B%22query%22%3A%20question%7D)%5B%22result%22%5D))%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%23%20%E5%B0%86%E9%97%AE%E7%AD%94%E7%BB%93%E6%9E%9C%E7%9B%B4%E6%8E%A5%E9%99%84%E5%8A%A0%E5%88%B0%E9%97%AE%E7%AD%94%E5%8E%86%E5%8F%B2%E4%B8%AD%EF%BC%8CGradio%20%E4%BC%9A%E5%B0%86%E5%85%B6%E5%B1%95%E7%A4%BA%E5%87%BA%E6%9D%A5%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20return%20%22%22%2C%20chat_history%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20except%20Exception%20as%20e%3A%0A%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20%C2%A0%20return%20e%2C%20chat_history%0A%60%60%60%0A%E7%84%B6%E5%90%8E%E6%88%91%E4%BB%AC%E5%8F%AA%E9%9C%80%E6%8C%89%E7%85%A7%20Gradio%20%E7%9A%84%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AE%9E%E4%BE%8B%E5%8C%96%E4%B8%80%E4%B8%AA%20Web%20%E7%95%8C%E9%9D%A2%E5%B9%B6%E5%B0%86%E7%82%B9%E5%87%BB%E5%8A%A8%E4%BD%9C%E7%BB%91%E5%AE%9A%E5%88%B0%E4%B8%8A%E8%BF%B0%E7%B1%BB%E7%9A%84%E5%9B%9E%E7%AD%94%E6%96%B9%E6%B3%95%E5%8D%B3%E5%8F%AF%EF%BC%9A%0A%E9%80%9A%E8%BF%87%E5%B0%86%E4%B8%8A%E8%BF%B0%E4%BB%A3%E7%A0%81%E5%B0%81%E8%A3%85%E4%B8%BA%20run_gradio.py%20%E8%84%9A%E6%9C%AC%EF%BC%8C%E7%9B%B4%E6%8E%A5%E9%80%9A%E8%BF%87%20python%20%E5%91%BD%E4%BB%A4%E8%BF%90%E8%A1%8C%EF%BC%8C%E5%8D%B3%E5%8F%AF%E5%9C%A8%E6%9C%AC%E5%9C%B0%E5%90%AF%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B%E7%9A%84%20Web%20Demo%EF%BC%8C%E9%BB%98%E8%AE%A4%E4%BC%9A%E5%9C%A8%207860%20%E7%AB%AF%E5%8F%A3%E8%BF%90%E8%A1%8C%EF%BC%8C%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%B0%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%E5%88%B0%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3%E5%8D%B3%E5%8F%AF%E8%AE%BF%E9%97%AE%3A%0A%0A%20%E5%9C%A8%E6%9C%AC%E5%9C%B0%E7%BB%88%E7%AB%AF%E8%BE%93%E5%85%A5%E4%BB%A5%E4%B8%8B%E6%8C%87%E4%BB%A4.7860%E6%98%AF%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E6%89%93%E5%BC%80%E7%9A%84%E7%AB%AF%E5%8F%A3%EF%BC%8C%E8%80%8C33090%E6%98%AF%E6%A0%B9%E6%8D%AE%E5%BC%80%E5%8F%91%E6%9C%BA%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BF%9B%E8%A1%8C%E6%9B%B4%E6%94%B9%E3%80%82%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%9A%0A%60%60%60shell%0Assh%20-CNg%20-L%207860%3A127.0.0.1%3A7860%20root%40ssh.intern-ai.org.cn%20-p%2033090%0A%60%60%60%0A%0A%23%23%206%20%E4%BD%9C%E4%B8%9A%0A%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8F%EF%BC%9A%E5%9C%A8%E5%90%84%E4%B8%AA%E7%8F%AD%E7%BA%A7%E5%AF%B9%E5%BA%94%E7%9A%84%20GitHub%20Discussion%20%E5%B8%96%E5%AD%90%E4%B8%AD%E8%BF%9B%E8%A1%8C%E6%8F%90%E4%BA%A4%E3%80%82%0A**%E5%9F%BA%E7%A1%80%E4%BD%9C%E4%B8%9A**%EF%BC%9A%0A%E5%A4%8D%E7%8E%B0%E8%AF%BE%E7%A8%8B%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%20%0A1%E3%80%81%E6%9E%84%E5%BB%BA%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%0A!%5B2518ef3fb12ef9236286d36e53ea3d54.png%5D(en-resource%3A%2F%2Fdatabase%2F5470%3A2)%0A2%E3%80%81qa%E9%93%BE%0A!%5B9fc815a730a7cf9b9c483652baab8e1a.png%5D(en-resource%3A%2F%2Fdatabase%2F5474%3A0)%0A3%E3%80%81%E5%9B%9E%E7%AD%94%0A!%5Bf37a61f8524977ac9fca9b966b4a5f58.png%5D(en-resource%3A%2F%2Fdatabase%2F5476%3A0)%0A%0A4%E3%80%81web%0A!%5B2917b35681e2b55df083a7273f83d840.png%5D(en-resource%3A%2F%2Fdatabase%2F5478%3A0)%0A%0A%0A**%E8%BF%9B%E9%98%B6%E4%BD%9C%E4%B8%9A**%EF%BC%9A%0A%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%EF%BC%8C%E6%94%B6%E9%9B%86%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E4%B8%9A%E8%B5%84%E6%96%99%E6%9E%84%E5%BB%BA%E4%B8%93%E4%B8%9A%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%B9%B6%E6%90%AD%E5%BB%BA%E4%B8%93%E4%B8%9A%E9%97%AE%E7%AD%94%E5%8A%A9%E6%89%8B%EF%BC%8C%E5%B9%B6%E5%9C%A8%20%5BOpenXLab%5D(https%3A%2F%2Fopenxlab.org.cn%2Fapps)%20%E4%B8%8A%E6%88%90%E5%8A%9F%E9%83%A8%E7%BD%B2%EF%BC%88%E6%88%AA%E5%9B%BE%EF%BC%8C%E5%B9%B6%E6%8F%90%E4%BE%9B%E5%BA%94%E7%94%A8%E5%9C%B0%E5%9D%80%EF%BC%89</center></span>
</div></body></html> 